The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /home/jeongseokoh/.cache/huggingface/token
Login successful
Inter SC
Num Path: 10 + 5 x 6
INFO 08-07 17:54:50 llm_engine.py:161] Initializing an LLM engine (v0.5.0.post1) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir='../../hub/model/', load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/data2/jeongseokoh/jeongseokoh/inter_SC/inter_SC_beam.py", line 806, in <module>
[rank0]:     main()
[rank0]:   File "/data2/jeongseokoh/jeongseokoh/inter_SC/inter_SC_beam.py", line 741, in main
[rank0]:     model = get_model(selected_model, cache_model, hf_token)
[rank0]:   File "/data2/jeongseokoh/jeongseokoh/inter_SC/inter_SC_beam.py", line 723, in get_model
[rank0]:     model = LLM(model="meta-llama/Meta-Llama-3-8B-Instruct", download_dir=cache_dir)
[rank0]:   File "/data2/jeongseokoh/miniconda3/envs/myenv/lib/python3.9/site-packages/vllm/entrypoints/llm.py", line 144, in __init__
[rank0]:     self.llm_engine = LLMEngine.from_engine_args(
[rank0]:   File "/data2/jeongseokoh/miniconda3/envs/myenv/lib/python3.9/site-packages/vllm/engine/llm_engine.py", line 363, in from_engine_args
[rank0]:     engine = cls(
[rank0]:   File "/data2/jeongseokoh/miniconda3/envs/myenv/lib/python3.9/site-packages/vllm/engine/llm_engine.py", line 223, in __init__
[rank0]:     self.model_executor = executor_class(
[rank0]:   File "/data2/jeongseokoh/miniconda3/envs/myenv/lib/python3.9/site-packages/vllm/executor/executor_base.py", line 41, in __init__
[rank0]:     self._init_executor()
[rank0]:   File "/data2/jeongseokoh/miniconda3/envs/myenv/lib/python3.9/site-packages/vllm/executor/gpu_executor.py", line 24, in _init_executor
[rank0]:     self.driver_worker.load_model()
[rank0]:   File "/data2/jeongseokoh/miniconda3/envs/myenv/lib/python3.9/site-packages/vllm/worker/worker.py", line 122, in load_model
[rank0]:     self.model_runner.load_model()
[rank0]:   File "/data2/jeongseokoh/miniconda3/envs/myenv/lib/python3.9/site-packages/vllm/worker/model_runner.py", line 148, in load_model
[rank0]:     self.model = get_model(
[rank0]:   File "/data2/jeongseokoh/miniconda3/envs/myenv/lib/python3.9/site-packages/vllm/model_executor/model_loader/__init__.py", line 21, in get_model
[rank0]:     return loader.load_model(model_config=model_config,
[rank0]:   File "/data2/jeongseokoh/miniconda3/envs/myenv/lib/python3.9/site-packages/vllm/model_executor/model_loader/loader.py", line 261, in load_model
[rank0]:     model = _initialize_model(model_config, self.load_config,
[rank0]:   File "/data2/jeongseokoh/miniconda3/envs/myenv/lib/python3.9/site-packages/vllm/model_executor/model_loader/loader.py", line 98, in _initialize_model
[rank0]:     return model_class(config=model_config.hf_config,
[rank0]:   File "/data2/jeongseokoh/miniconda3/envs/myenv/lib/python3.9/site-packages/vllm/model_executor/models/llama.py", line 340, in __init__
[rank0]:     self.model = LlamaModel(config,
[rank0]:   File "/data2/jeongseokoh/miniconda3/envs/myenv/lib/python3.9/site-packages/vllm/model_executor/models/llama.py", line 262, in __init__
[rank0]:     self.layers = nn.ModuleList([
[rank0]:   File "/data2/jeongseokoh/miniconda3/envs/myenv/lib/python3.9/site-packages/vllm/model_executor/models/llama.py", line 263, in <listcomp>
[rank0]:     LlamaDecoderLayer(config=config,
[rank0]:   File "/data2/jeongseokoh/miniconda3/envs/myenv/lib/python3.9/site-packages/vllm/model_executor/models/llama.py", line 200, in __init__
[rank0]:     self.mlp = LlamaMLP(
[rank0]:   File "/data2/jeongseokoh/miniconda3/envs/myenv/lib/python3.9/site-packages/vllm/model_executor/models/llama.py", line 64, in __init__
[rank0]:     self.gate_up_proj = MergedColumnParallelLinear(
[rank0]:   File "/data2/jeongseokoh/miniconda3/envs/myenv/lib/python3.9/site-packages/vllm/model_executor/layers/linear.py", line 348, in __init__
[rank0]:     super().__init__(input_size=input_size,
[rank0]:   File "/data2/jeongseokoh/miniconda3/envs/myenv/lib/python3.9/site-packages/vllm/model_executor/layers/linear.py", line 252, in __init__
[rank0]:     self.quant_method.create_weights(
[rank0]:   File "/data2/jeongseokoh/miniconda3/envs/myenv/lib/python3.9/site-packages/vllm/model_executor/layers/linear.py", line 94, in create_weights
[rank0]:     weight = Parameter(torch.empty(sum(output_partition_sizes),
[rank0]:   File "/data2/jeongseokoh/miniconda3/envs/myenv/lib/python3.9/site-packages/torch/utils/_device.py", line 78, in __torch_function__
[rank0]:     return func(*args, **kwargs)
[rank0]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 
srun: error: n02: task 0: Exited with exit code 1
