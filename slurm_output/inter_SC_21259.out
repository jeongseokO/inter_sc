The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /home/jeongseokoh/.cache/huggingface/token
Login successful
Inter SC_llm
Num Path: 10 + 5 x 2, Max steps: 12, Distance: levenshtein, Clustering: agglomerative
INFO 08-13 20:09:53 llm_engine.py:161] Initializing an LLM engine (v0.5.0.post1) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir='../../hub/model/', load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct)
INFO 08-13 20:09:57 weight_utils.py:218] Using model weights format ['*.safetensors']
INFO 08-13 20:10:01 model_runner.py:160] Loading model weights took 14.9595 GB
INFO 08-13 20:10:03 gpu_executor.py:83] # GPU blocks: 13407, # CPU blocks: 2048
INFO 08-13 20:10:06 model_runner.py:889] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 08-13 20:10:06 model_runner.py:893] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 08-13 20:10:16 model_runner.py:965] Graph capturing finished in 10 secs.
Model: llama3_8b is selected
Dataset: gsm8k is selected
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.36it/s, est. speed input: 1585.16 toks/s, output: 272.36 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.36it/s, est. speed input: 1585.16 toks/s, output: 272.36 toks/s]
Generation job: 0.7458233269862831 sec
Added Probs: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Cluster Indices: {0: [0, 1, 2, 3, 4, 5], 1: [9], 2: [8], 3: [7], 4: [6]}
Selected Indexes: [0, 9, 8, 7, 6]
Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  20%|██        | 1/5 [00:01<00:06,  1.57s/it, est. speed input: 755.27 toks/s, output: 36.36 toks/s]Processed prompts: 100%|██████████| 5/5 [00:01<00:00,  3.19it/s, est. speed input: 3775.41 toks/s, output: 181.75 toks/s]
Generation job: 1.5795342929195613 sec
Added Probs: [-0.025551346751550835, -0.030321749497433097, -0.025551346751550835, -0.030321749497433097, -0.025551346751550835, -0.030321749497433097, -0.025551346751550835, -0.030321749497433097, -0.025551346751550835, -0.030321749497433097]
Cluster Indices: {0: [0, 2, 4, 6], 1: [1, 3, 5], 2: [9], 3: [8], 4: [7]}
Selected Indexes: [0, 1, 9, 8, 7]
step: 2
Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  20%|██        | 1/5 [00:01<00:06,  1.63s/it, est. speed input: 745.52 toks/s, output: 36.91 toks/s]Processed prompts: 100%|██████████| 5/5 [00:01<00:00,  3.02it/s, est. speed input: 3662.82 toks/s, output: 183.14 toks/s]
Generation job: 1.666431280085817 sec
Added Probs: [-0.02633760391901701, -0.029542732124145214, -0.03389714695513248, -0.03713822402531588, -0.03389714695513248, -0.03713822402531588, -0.02633760391901701, -0.029542732124145214, -0.03389714695513248, -0.03713822402531588]
Cluster Indices: {0: [2, 4, 8], 1: [3, 5], 2: [1, 7], 3: [0, 6], 4: [9]}
Selected Indexes: [2, 3, 1, 0, 9]
step: 3
Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  20%|██        | 1/5 [00:01<00:05,  1.49s/it, est. speed input: 832.18 toks/s, output: 33.50 toks/s]Processed prompts:  80%|████████  | 4/5 [00:01<00:00,  2.77it/s, est. speed input: 2785.40 toks/s, output: 121.71 toks/s]Processed prompts: 100%|██████████| 5/5 [00:01<00:00,  2.80it/s, est. speed input: 3481.71 toks/s, output: 159.28 toks/s]
Generation job: 1.794943424174562 sec
Added Probs: [-0.03555184616929009, -0.03555184616929009, -0.04171950083512526, -0.05251778117739237, -0.02981109479698566, -0.036266239196584935, -0.024130940574024796, -0.03523810090203034, -0.04171950083512526, -0.05251778117739237]
Cluster Indices: {0: [4, 6], 1: [2, 3, 8, 9], 2: [0, 1], 3: [7], 4: [5]}
Selected Indexes: [6, 2, 0, 7, 5]
step: 4
Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  20%|██        | 1/5 [00:01<00:04,  1.17s/it, est. speed input: 1091.55 toks/s, output: 22.29 toks/s]Processed prompts:  80%|████████  | 4/5 [00:01<00:00,  3.26it/s, est. speed input: 3427.56 toks/s, output: 86.13 toks/s]Processed prompts: 100%|██████████| 5/5 [00:01<00:00,  3.36it/s, est. speed input: 4278.94 toks/s, output: 119.76 toks/s]
Generation job: 1.4984275770839304 sec
Added Probs: [-0.02155961084072707, -0.02155961084072707, -0.044929441093474395, -0.044929441093474395, -0.040677583332245165, -0.040677583332245165, -0.03163105120339731, -0.03163105120339731, -0.032553946995359705, -0.032553946995359705]
Cluster Indices: {0: [4, 5], 1: [8, 9], 2: [2, 3], 3: [0, 1], 4: [6, 7]}
Selected Indexes: [4, 8, 2, 0, 6]
step: 5
Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  20%|██        | 1/5 [00:00<00:03,  1.20it/s, est. speed input: 1551.62 toks/s, output: 2.40 toks/s]Processed prompts:  80%|████████  | 4/5 [00:01<00:00,  4.49it/s, est. speed input: 4806.65 toks/s, output: 23.30 toks/s]Processed prompts: 100%|██████████| 5/5 [00:01<00:00,  4.66it/s, est. speed input: 6007.05 toks/s, output: 41.93 toks/s]
Generation job: 1.0853525940328836 sec
Added Probs: [-0.03921070833291326, -0.05028416709505397, -0.032553946995359705, -0.032553946995359705, -0.041697107201857535, -0.041697107201857535, -0.02155961084072707, -0.02155961084072707, -0.03163105120339731, -0.03163105120339731]
Cluster Indices: {0: [0, 1], 1: [6, 7], 2: [4, 5], 3: [8, 9], 4: [2, 3]}
Selected Indexes: [0, 6, 4, 8, 2]
step: 6
Found EoS token, Break
completions: ["<|start_header_id|>assistant<|end_header_id|>\n\nStep 1: Janet's ducks lay 16 eggs per day. ******\n\nStep 2: She eats 3 eggs for breakfast, so she has 16 - 3 = 13 eggs left. ******\n\nStep 3: She bakes 4 eggs for muffins, so she has 13 - 4 = 9 eggs left. ******\n\nStep 4: She sells the remaining 9 eggs at the farmers' market for $2 per egg. ******\n\nStep 5: The total amount she makes is 9 eggs x $2 per egg = $18. ******\n\nThe answer is #### 18. ******", "<|start_header_id|>assistant<|end_header_id|>\n\nStep 1: Janet's ducks lay 16 eggs per day. ****** Step 2: She eats 3 eggs for breakfast, so she has 16 - 3 = 13 eggs left. ****** Step 3: She bakes 4 eggs for muffins, so she has 13 - 4 = 9 eggs left. ****** Step 4: She sells the remaining 9 eggs at $2 per egg, so she makes 9 * $2 = $18. ****** Step 5: The answer is #### 18. ******", "<|start_header_id|>assistant<|end_header_id|>\n\nStep 1: Janet's ducks lay 16 eggs per day. ******\n\nStep 2: She eats 3 eggs for breakfast, so she has 16 - 3 = 13 eggs left. ******\n\nStep 3: She bakes muffins with 4 eggs, leaving her with 13 - 4 = 9 eggs. ******\n\nStep 4: She sells the remaining 9 eggs at the farmers' market for $2 per egg. ******\n\nStep 5: The total amount she makes is 9 eggs x $2 per egg = $18. ******\n\nThe answer is #### 18. ******", "<|start_header_id|>assistant<|end_header_id|>\n\nStep 1: Janet's ducks lay 16 eggs per day. ****** Step 2: She eats 3 eggs for breakfast, so she has 16 - 3 = 13 eggs left. ****** Step 3: She bakes 4 eggs for muffins, so she has 13 - 4 = 9 eggs left. ****** Step 4: She sells the remaining 9 eggs at the farmers' market for $2 per egg, so she makes 9 * $2 = $18. ****** Step 5: The answer is #### 18. ******", "<|start_header_id|>assistant<|end_header_id|>\n\nStep 1: Janet's ducks lay 16 eggs per day. ****** Step 2: She eats 3 eggs for breakfast, so she has 16 - 3 = 13 eggs left. ****** Step 3: She bakes muffins with 4 eggs, so she has 13 - 4 = 9 eggs left. ****** Step 4: She sells the remaining 9 eggs at the farmers' market for $2 per egg, so she makes 9 * $2 = $18. ****** Step 5: The answer is #### 18. ******"]
Answer Probs: []
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.55it/s, est. speed input: 2065.20 toks/s, output: 30.56 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.55it/s, est. speed input: 2065.20 toks/s, output: 30.56 toks/s]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/data2/jeongseokoh/jeongseokoh/inter_SC/inter_SC_llm_to_llm.py", line 1028, in <module>
[rank0]:     #os.environ["CUDA_LAUNCH_BLOCKING"] = "1"
[rank0]:   File "/data2/jeongseokoh/jeongseokoh/inter_SC/inter_SC_llm_to_llm.py", line 1013, in main
[rank0]:     total = 0 
[rank0]:   File "/data2/jeongseokoh/jeongseokoh/inter_SC/inter_SC_llm_to_llm.py", line 633, in parallel_process
[rank0]:     extracted_real_answer = extract_string(real_answer)
[rank0]:   File "/data2/jeongseokoh/jeongseokoh/inter_SC/inter_SC_llm_to_llm.py", line 453, in get_completion
[rank0]:     final_generated_texts = [text.text if len(text.text) > 2 else text.text for output in final_outputs for text in output.final_outputs]
[rank0]:   File "/data2/jeongseokoh/jeongseokoh/inter_SC/inter_SC_llm_to_llm.py", line 453, in <listcomp>
[rank0]:     final_generated_texts = [text.text if len(text.text) > 2 else text.text for output in final_outputs for text in output.final_outputs]
[rank0]: AttributeError: 'RequestOutput' object has no attribute 'final_outputs'
srun: error: n02: task 0: Exited with exit code 1
